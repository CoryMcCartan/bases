[{"path":"http://corymccartan.com/bases/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Cory McCartan Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"http://corymccartan.com/bases/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Cory McCartan. Author, maintainer.","code":""},{"path":"http://corymccartan.com/bases/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"McCartan C (2025). bases: Basis Expansions Regression Modeling. R package version 0.0.0.9000, http://corymccartan.com/bases/.","code":"@Manual{,   title = {bases: Basis Expansions for Regression Modeling},   author = {Cory McCartan},   year = {2025},   note = {R package version 0.0.0.9000},   url = {http://corymccartan.com/bases/}, }"},{"path":"http://corymccartan.com/bases/index.html","id":"bases-","dir":"","previous_headings":"","what":"Basis Expansions for Regression Modeling","title":"Basis Expansions for Regression Modeling","text":"bases provides various basis expansions flexible regression modeling, including random Fourier features, exact kernel / Gaussian process feature maps, BART prior features, helpful interface n-way interactions. provided functions may used within modeling formula, allowing use kernel methods basis expansions modeling functions otherwise support . Along basis expansions, number kernel functions also provided, support kernel arithmetic form new kernels. Basic ridge regression functionality included well.","code":""},{"path":"http://corymccartan.com/bases/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Basis Expansions for Regression Modeling","text":"can install development version bases :","code":"remotes::install_github(\"CoryMcCartan/bases\")"},{"path":"http://corymccartan.com/bases/index.html","id":"example-random-fourier-features","dir":"","previous_headings":"","what":"Example: random Fourier features","title":"Basis Expansions for Regression Modeling","text":"basis functions bases start b_ designed work way built-basis expansions like splines::bs() poly(): simply include function model formula. fitting approximate kernel regression random Fourier features simple wrapping relevant variables call corresponding basis function, b_rff(). default kernel Gaussian/RBF kernel length scale 1 applied predictors rescaling unit variance. can provide different kernel = argument switch kernels. Many common kernels provided package; see ?kernels. practice, RFF usually fit penalization, via ridge regression. , visualize several RFF ridge regression fits versus simple linear model, using ridge() function provided package.","code":"library(bases)  # Box & Jenkins (1976) sales data x = 1:150 y = as.numeric(BJsales)   lm(y ~ b_rff(x, p = 5)) # 5 random features #>  #> Call: #> lm(formula = y ~ b_rff(x, p = 5)) #>  #> Coefficients: #>      (Intercept)  b_rff(x, p = 5)1  b_rff(x, p = 5)2  b_rff(x, p = 5)3   #>         -1821607             12882             33383            -35639   #> b_rff(x, p = 5)4  b_rff(x, p = 5)5   #>            88853           4123981 b_rff(x, kernel = k_matern(scale = 0.1, nu = 5/2)) b_rff(x, kernel = k_rq(scale = 2, alpha = 2)) k = k_rbf(scale = 0.2) plot(x, y, xlab = \"Month\", ylab = \"Sales\") lines(x, fitted(lm(y ~ x)), lty = \"dashed\", lwd = 1.5) for (i in 1:20) {     m_rff = ridge(y ~ b_rff(x, kernel = k))     lines(x, fitted(m_rff), col = \"#4584\") }"},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Additive Regression Tree (BART) features — b_bart","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"Generates random features BART prior symmetric trees. Equivalently, features interaction small number indicator functions. number interacted indicators depth symmetric tree, drawn prior tree depth calibrated match traditional BART prior Chipman et al. (2010). variable tree node selected uniformly, thresholds selected uniformly range variable.","code":""},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"","code":"b_bart(   ...,   trees = 100,   depths = bart_depth_prior()(trees),   vars = NULL,   thresh = NULL,   drop = NULL,   ranges = NULL )  bart_depth_prior(mean_depth = 1.25)"},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"... variable(s) build features . single data frame matrix may provided well. Missing values allowed. trees number trees sample. depths depths tree. default, drawn Poisson distribution calibrated produce trees around 2.5 leaves, average, matching traditional BART prior. vars Integer indices variables use tree. provided, overrides generated automatically sampling uniformly available input features. Provided flat form, length equal sum(depths). thresh thresholds variable. provided, overrides generated automatically sampling uniformly ranges, defaults range input feature. Provided flat form, length equal sum(depths). drop Columns calculated indicator matrix drop. default, leaves match zero input rows dropped.  provided, overrides default. ranges range input features, provided matrix two rows column input feature. first row minimum second row maximum. mean_depth mean prior depth tree, single node depth zero two-leaf tree depth 1. value minus one becomes rate parameter Poisson distribution, whose samples shifted one. way, zero-depth trees (produce trivial features) sampled.","code":""},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"matrix indicator variables encoding random features.","code":""},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"bart_depth_prior(): Poisson depth prior random trees, parametrized terms mean tree depth. Returns function generates samples prior argument giving number samples. default prior closely matches average number leaves original (asymmetric) BART prior.","code":""},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"Hugh . Chipman. Edward . George. Robert E. McCulloch. \"BART: Bayesian additive regression trees.\" Ann. Appl. Stat. 4 (1) 266 - 298, March 2010. https://doi.org/10.1214/09-AOAS285","code":""},{"path":"http://corymccartan.com/bases/reference/b_bart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Additive Regression Tree (BART) features — b_bart","text":"","code":"X = with(mtcars, b_bart(cyl, disp, hp, drat, wt, trees = 50)) all(colSums(X) > 0) # TRUE; empty leaves are pruned away #> [1] TRUE # each row belongs to 1 leaf node per tree; some trees pruned away all(rowSums(X) == rowSums(X)[1]) # TRUE #> [1] TRUE all(rowSums(X) <= 50) # TRUE #> [1] TRUE  x = 1:150 y = as.numeric(BJsales) m = ridge(y ~ b_bart(x, trees=25)) plot(x, y) lines(x, fitted(m), type=\"s\", col=\"blue\")"},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":null,"dir":"Reference","previous_headings":"","what":"N-way interaction basis — b_inter","title":"N-way interaction basis — b_inter","text":"Generates design matrix contains possible interactions input variables specified maximum depth. default \"symbox\" standardization, maps inputs \\([-0.5, 0.5]^d\\), strongly recommended, means interaction terms smaller variance thus penalized methods like Lasso ridge regression (see Gelman et al., 2008).","code":""},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"N-way interaction basis — b_inter","text":"","code":"b_inter(   ...,   depth = 2,   stdize = c(\"symbox\", \"box\", \"scale\", \"none\"),   shift = NULL,   scale = NULL )"},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"N-way interaction basis — b_inter","text":"... variable(s) build features . single data frame matrix may provided well. Missing values allowed. depth maximum interaction depth. default 2, means pairwise interactions included. stdize standardize predictors, . default \"scale\" applies scale() input features mean zero unit variance, \"box\" scales data along dimension lie unit hypercube, \"symbox\" scales data along dimension lie \\([-0.5, 0.5]^d\\). shift Vector shifts, single shift value, use. provided, overrides calculated according stdize. scale Vector scales, single scale value, use. provided, overrides calculated according stdize.","code":""},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"N-way interaction basis — b_inter","text":"matrix rescaled interacted features.","code":""},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"N-way interaction basis — b_inter","text":"Gelman, ., Jakulin, ., Pittau, M. G., & Su, Y. S. (2008). weakly informative default prior distribution logistic regression models.","code":""},{"path":"http://corymccartan.com/bases/reference/b_inter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"N-way interaction basis — b_inter","text":"","code":"# default: all pairwise interactions lm(mpg ~ b_inter(cyl, hp, wt), mtcars) #>  #> Call: #> lm(formula = mpg ~ b_inter(cyl, hp, wt), data = mtcars) #>  #> Coefficients: #>                (Intercept)     b_inter(cyl, hp, wt)cyl   #>                     15.225                       2.914   #>     b_inter(cyl, hp, wt)hp      b_inter(cyl, hp, wt)wt   #>                    -11.443                     -13.041   #> b_inter(cyl, hp, wt)cyl:hp  b_inter(cyl, hp, wt)cyl:wt   #>                     13.724                       6.855   #>  b_inter(cyl, hp, wt)hp:wt   #>                      7.050   #>   # how number of features depends on interaction depth for (d in 2:6) {     X = with(mtcars, b_inter(cyl, disp, hp, drat, wt, depth=d))     print(ncol(X)) } #> [1] 15 #> [1] 25 #> [1] 30 #> [1] 31 #> [1] 31"},{"path":"http://corymccartan.com/bases/reference/b_ker.html","id":null,"dir":"Reference","previous_headings":"","what":"Exact kernel feature basis — b_ker","title":"Exact kernel feature basis — b_ker","text":"Generates design matrix exactly represents provided kernel, Gram matrix equal kernel matrix. feature map $$      \\phi(x') = K_{x,x}^{-1/2} k_{x,x'}, $$ \\(K_{x,x}\\) kernel matrix data points \\(x\\) \\(k_{x, x'}\\) vector kernel function evaluations data points new value. exact, function particularly computationally efficient. fitting prediction require backsolving Cholesky decomposition kernel matrix original data points.","code":""},{"path":"http://corymccartan.com/bases/reference/b_ker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exact kernel feature basis — b_ker","text":"","code":"b_ker(   ...,   kernel = k_rbf(),   stdize = c(\"scale\", \"box\", \"symbox\", \"none\"),   x = NULL,   shift = NULL,   scale = NULL )"},{"path":"http://corymccartan.com/bases/reference/b_ker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exact kernel feature basis — b_ker","text":"... variable(s) build features . single data frame matrix may provided well. Missing values allowed. kernel kernel function. one recognized kernel functions k_rbf() provided, computations exact. Otherwise, fast Fourier transform provided kernel function used generate random features. kernel shift-invariant decay zero positive negative infinity. stdize standardize predictors, . default \"scale\" applies scale() input features mean zero unit variance, \"box\" scales data along dimension lie unit hypercube, \"symbox\" scales data along dimension lie \\([-0.5, 0.5]^d\\). x (training) data points evaluate kernel. provided, overrides .... shift Vector shifts, single shift value, use. provided, overrides calculated according stdize. scale Vector scales, single scale value, use. provided, overrides calculated according stdize.","code":""},{"path":"http://corymccartan.com/bases/reference/b_ker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exact kernel feature basis — b_ker","text":"matrix kernel features.","code":""},{"path":"http://corymccartan.com/bases/reference/b_ker.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Exact kernel feature basis — b_ker","text":"","code":"data(quakes)  # exact kernel ridge regression k = k_rbf(0.1) m = ridge(depth ~ b_ker(lat, long, kernel = k), quakes) cor(fitted(m), quakes$depth)^2 #> [1] 0.9668987  # Forecasting example involving combined kernels data(AirPassengers) x = seq(1949, 1961 - 1/12, 1/12) y = as.numeric(AirPassengers) x_pred = seq(1961 - 1/2, 1965, 1/12)  k = k_per(scale = 0.2, period = 1) * k_rbf(scale = 4) m = ridge(y ~ b_ker(x, kernel = k, stdize=\"none\")) plot(x, y, type='l', xlab=\"Year\", ylab=\"Passengers (thousands)\",     xlim=c(1949, 1965), ylim=c(100, 800)) lines(x_pred, predict(m, newdata = list(x = x_pred)), lty=\"dashed\")"},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Fourier feature basis — b_rff","title":"Random Fourier feature basis — b_rff","text":"Generates random Fourier feature basis matrix provided kernel, optionally rescaling data lie unit hypercube. good review random features Liu et al. (2021) review paper cited . Random features form $$   \\phi(x) = \\cos(\\omega^T x + b), $$ \\(\\omega\\) vector frequencies sampled Fourier transform kernel, \\(b\\sim\\mathrm{Unif}[-\\pi, \\pi]\\) random phase shift. input data x may shifted rescaled feature mapping applied, according stdize argument.","code":""},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Fourier feature basis — b_rff","text":"","code":"b_rff(   ...,   p = 100,   kernel = k_rbf(),   stdize = c(\"scale\", \"box\", \"symbox\", \"none\"),   n_approx = nextn(4 * p),   freqs = NULL,   phases = NULL,   shift = NULL,   scale = NULL )"},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Fourier feature basis — b_rff","text":"... variable(s) build features . single data frame matrix may provided well. Missing values allowed. p number random features. kernel kernel function. one recognized kernel functions k_rbf() provided, computations exact. Otherwise, fast Fourier transform provided kernel function used generate random features. kernel shift-invariant decay zero positive negative infinity. stdize standardize predictors, . default \"scale\" applies scale() input features mean zero unit variance, \"box\" scales data along dimension lie unit hypercube, \"symbox\" scales data along dimension lie \\([-0.5, 0.5]^d\\). n_approx number discrete frequencies use calculating Fourier transform provided kernel.  used certain kernels analytic Fourier transform available; see . freqs Matrix frequencies use; ncol(freqs) must match number predictors. provided, overrides calculated automatically, thus ignoring p kernel. phases Vector phase shifts use. provided, overrides calculated automatically, thus ignoring p kernel. shift Vector shifts, single shift value, use. provided, overrides calculated according stdize. scale Vector scales, single scale value, use. provided, overrides calculated according stdize.","code":""},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random Fourier feature basis — b_rff","text":"matrix random Fourier features.","code":""},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Random Fourier feature basis — b_rff","text":"reduce variance approximation, moment-matching transformation applied ensure sampled frequencies mean zero, per Shen et al. (2017).  Gaussian/RBF kernel, second moment-matching also applied ensure analytical empirical frequency covariance matrices agree.","code":""},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Random Fourier feature basis — b_rff","text":"Rahimi, ., & Recht, B. (2007). Random features large-scale kernel machines. Advances Neural Information Processing Systems, 20. Liu, F., Huang, X., Chen, Y., & Suykens, J. . (2021). Random features kernel approximation: survey algorithms, theory, beyond. IEEE Transactions Pattern Analysis Machine Intelligence, 44(10), 7128-7148. Shen, W., Yang, Z., & Wang, J. (2017, February). Random features shift-invariant kernels moment matching. Proceedings AAAI Conference Artificial Intelligence (Vol. 31, . 1).","code":""},{"path":"http://corymccartan.com/bases/reference/b_rff.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random Fourier feature basis — b_rff","text":"","code":"data(quakes)  m = ridge(depth ~ b_rff(lat, long), quakes) plot(fitted(m), quakes$depth)   # more random featues means a higher ridge penalty m500 = ridge(depth ~ b_rff(lat, long, p = 500), quakes) c(default = m$penalty, p500 = m500$penalty) #>      default         p500  #> 9.637751e-07 4.504327e-05   # A shorter length scale fits the data better (R^2) m_025 = ridge(depth ~ b_rff(lat, long, kernel = k_rbf(scale = 0.25)), quakes) c(   len_1 = cor(quakes$depth, fitted(m))^2,   len_025 = cor(quakes$depth, fitted(m_025))^2 ) #>     len_1   len_025  #> 0.9179528 0.9399802"},{"path":"http://corymccartan.com/bases/reference/bases-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bases: Basis Expansions for Regression Modeling — bases-package","title":"bases: Basis Expansions for Regression Modeling — bases-package","text":"Provides various basis expansions flexible regression modeling, including random Fourier features (Rahimi & Recht, 2007), exact kernel / Gaussian process feature maps, BART (Chipman et al., 2010) doi:10.1214/09-AOAS285  prior features, helpful interface n-way interactions. provided functions may used within modeling formula, allowing use kernel methods basis expansions modeling functions otherwise support . Along basis expansions, number kernel functions also provided, support kernel arithmetic form new kernels. Basic ridge regression functionality included well.","code":""},{"path":[]},{"path":"http://corymccartan.com/bases/reference/bases-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bases: Basis Expansions for Regression Modeling — bases-package","text":"Maintainer: Cory McCartan mccartan@psu.edu (ORCID)","code":""},{"path":"http://corymccartan.com/bases/reference/kernel-arith.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel arithmetic — kernel-arith","title":"Kernel arithmetic — kernel-arith","text":"Kernel functions (see ?kernels) may multiplied constants, multiplied , added together.","code":""},{"path":"http://corymccartan.com/bases/reference/kernel-arith.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel arithmetic — kernel-arith","text":"","code":"# S3 method for class 'kernel' x * k2  # S3 method for class 'kernel' k1 + k2"},{"path":"http://corymccartan.com/bases/reference/kernel-arith.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel arithmetic — kernel-arith","text":"x numeric kernel function k2 kernel function k1 kernel function","code":""},{"path":"http://corymccartan.com/bases/reference/kernel-arith.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel arithmetic — kernel-arith","text":"new kernel function, class c(\"kernel\", \"function\").","code":""},{"path":"http://corymccartan.com/bases/reference/kernel-arith.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel arithmetic — kernel-arith","text":"","code":"x = seq(-1, 1, 0.5) k = k_rbf() k2 = k_per(scale=0.2, period=0.3)  k_add = k2 + 0.5*k print(k_add) #> function (x, y)  #> { #>     k1(x, y) + k2(x, y) #> } #> <bytecode: 0x562d5f2ef328> #> <environment: 0x562d5f2efe50> #> attr(,\"name\") #> [1] \"per + rbf\" image(k_add(x, x))"},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel functions — kernels","title":"Kernel functions — kernels","text":"functions return vectorized kernel functions can used calculate kernel matrices, provided directly basis functions. functions designed take maximum value one identical inputs provided. Kernels can combined arithmetic expressions; see ?kernel-arith.","code":""},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel functions — kernels","text":"","code":"k_rbf(scale = 1)  k_lapl(scale = 1)  k_rq(scale = 1, alpha = 2)  k_matern(scale = 1, nu = 1.5)  k_per(scale = 1, period = 1)"},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel functions — kernels","text":"scale kernel length scale. alpha shape/df parameter. \\(\\alpha=1\\) Cauchy kernel. nu smoothness parameter. \\(\\nu=0.5\\) Ornstein–Uhlenbeck kernel. period period, units scale.","code":""},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel functions — kernels","text":"function calculates kernel matrix vector arguments x y. function class c(\"kernel\", \"function\").","code":""},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Kernel functions — kernels","text":"k_rbf(): Radial basis function kernel k_lapl(): Laplace kernel k_rq(): Rational quadratic kernel. k_matern(): Matérn kernel. k_per(): Periodic (exp-sine-squared) kernel.","code":""},{"path":"http://corymccartan.com/bases/reference/kernels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel functions — kernels","text":"","code":"k = k_rbf() x = seq(-1, 1, 0.5) k(0, 0) #>      [,1] #> [1,]    1 k(0, x) #>           [,1]      [,2] [,3]      [,4]      [,5] #> [1,] 0.6065307 0.8824969    1 0.8824969 0.6065307 k(x, x) #>           [,1]      [,2]      [,3]      [,4]      [,5] #> [1,] 1.0000000 0.8824969 0.6065307 0.3246525 0.1353353 #> [2,] 0.8824969 1.0000000 0.8824969 0.6065307 0.3246525 #> [3,] 0.6065307 0.8824969 1.0000000 0.8824969 0.6065307 #> [4,] 0.3246525 0.6065307 0.8824969 1.0000000 0.8824969 #> [5,] 0.1353353 0.3246525 0.6065307 0.8824969 1.0000000  k = k_per(scale=0.2, period=0.3) round(k(x, x)) #>      [,1] [,2] [,3] [,4] [,5] #> [1,]    1    0    0    1    0 #> [2,]    0    1    0    0    1 #> [3,]    0    0    1    0    0 #> [4,]    1    0    0    1    0 #> [5,]    0    1    0    0    1"},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":null,"dir":"Reference","previous_headings":"","what":"Ridge regression — ridge","title":"Ridge regression — ridge","text":"Lightweight routine ridge regression, fitted via singular value decomposition. penalty may automatically determined leave-one-cross validation. intercept term unpenalized.","code":""},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ridge regression — ridge","text":"","code":"ridge(formula, data, penalty = \"auto\", ...)  # S3 method for class 'ridge' fitted(object, ...)  # S3 method for class 'ridge' coef(object, ...)  # S3 method for class 'ridge' predict(object, newdata, ...)"},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ridge regression — ridge","text":"formula model formula; see formula. intercept term unpenalized; fit penalized intercept, remove intercept add design matrix. data optional data frame object interpret variables occurring formula. penalty ridge penalty. Must single numeric string \"auto\", case penalty determined via leave-one-cross validation minimize mean squared error. ... arguments, passed model.frame() model.matrix(). must provided predict.ridge() well, used. object fitted ridge() model. newdata data frame containing new data predict","code":""},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ridge regression — ridge","text":"object class ridge components including: coef, vector coefficients. fitted, vector fitted values. penalty, penalty value.","code":""},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":"methods-by-generic-","dir":"Reference","previous_headings":"","what":"Methods (by generic)","title":"Ridge regression — ridge","text":"fitted(ridge): Fitted values coef(ridge): Coefficients predict(ridge): Predicted values","code":""},{"path":"http://corymccartan.com/bases/reference/ridge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ridge regression — ridge","text":"","code":"m_lm = lm(mpg ~ ., mtcars) m_ridge = ridge(mpg ~ ., mtcars, penalty=1e3) plot(fitted(m_lm), fitted(m_ridge), ylim=c(10, 30)) abline(a=0, b=1, col=\"red\")"},{"path":"http://corymccartan.com/bases/news/index.html","id":"bases-010","dir":"Changelog","previous_headings":"","what":"bases 0.1.0","title":"bases 0.1.0","text":"Basis expansions Gaussian processes / kernel ridge regression, random Fourier features, BART prior features, n-way interactions Lightweight ridge regression routine Gaussian, Laplace, Rational quadratic, Matérn, periodic kernels","code":""}]
